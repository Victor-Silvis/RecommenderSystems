{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1> **`KNN Collaborative` Recommender System `item-based`** </h1>  \n",
    "_Code by Victor Silvis_   \n",
    "_500777168_ \n",
    "  \n",
    "This notebook contains two versions of KNN collaborative Filtering recommender systems. One based on the traditional concept of only using item-user rating interactions, and the second one adds additional item features on top of these ratings, to try and get a more accurate rating prediction. Both of them have two types of predictions methodologies, regression and classification, which can be set in the recommenders system initialisation.  \n",
    "  \n",
    "**RecSys In Notebook:**\n",
    "1. KNN CF ITEM-BASED with only ratings (Classification or Regression)  \n",
    "2. KNN CF ITEM-BASED with ratings AND features (Classification or Regression)  \n",
    "  \n",
    "**Contents:**\n",
    "1. Introduction Item-Based\n",
    "2. Loading Data\n",
    "3. Recommender system 1 KNN CF without features\n",
    "4. Recommender system 2 KNN CF with features\n",
    "5. Comparison of results\n",
    "6. Conclusion\n",
    "  \n",
    "Datasets: Movielens & Netflix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction Item-Based**  \n",
    "In contrary to the user-based approach of the other KNN notebook, the recommendations are based upon finding similar movies, based on the rating patterns. So as an example, Instead of consulting with his peers, Eric instead determines whether the movie “Titanic” is right for him by considering the movies that he has already seen. He notices that people that have rated this movie have given similar ratings to the movies “Forrest Gump” and “Wall-E”. Since Eric liked these two movies he concludes that he will also like the movie “Titanic”. This concept will be used in this system. Similar items will be determined based on the favorite item of a user. We then check  the user rated items that are similar to the ones we are going to recommend. These ratings in combination with the distance are used to give a weighted predicted rating for movies the user hasn't seen yet. The recommender systems in this notebook take an user like Eric as input, and recommends items based on his personal favorite item. \n",
    "  \n",
    "**Regression vs Classification**  \n",
    "Secondly, based on (Nikolakopoulos et al. 2021), there a two methodologies of predicting the rating, classification and regression. For regression we take the ratings from Eric for Forrest Gump and Wall-E, and together with the similarity (distance) to the Titanic, we calculate a weighted score the the Titanic for Eric. With classification we dont calculate a weighted score, we classify an item (in this case Forrest Gump or Wall-E) as the top voter, and we take that rating directly, as this item is the most similar to the Titanic. Choosing the rating that is closest, and classify that one as the most appropriate (weight 1 or 0), has been proven to be a risky but viable approach in literature (Nikolakopoulos et al. 2021). This is one of the things this notebook will analyse.  \n",
    "  \n",
    "**Features vs without item Features**  \n",
    "Finally this notebook will compare two systems. The first one will be solely based on the rating patterns, and utilising it to find similar items. A proven traditional approach of collaborative filtering. But the second system will also include the item features, in this case the genre. It will resemble more like a hybrid system (incoperating aspects of content-based). This notebook will analyse if this more complex system will also perform better in terms of accuracy and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Packages and Data**  \n",
    "First the necessary packages are imported. Together with packages from the selfmade utils folder for distance calculations. In addition the netflix and movielens datasets are imported (see notebook about sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#Import package from own folder\n",
    "from Utils.distance import Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Movielens (~100.000 Rows)\n",
    "ratings_ml = pd.read_csv('../../Data/Movielens/ratings.csv')\n",
    "movies_ml = pd.read_csv('../../Data/Movielens/movies.csv')\n",
    "ratings_ml.columns = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "\n",
    "#Data Netflix (~400.000 rows)\n",
    "ratings_nf = pd.read_parquet('../../Data/Netflix/NetflixSample.gzip')\n",
    "ratings_nf[['user_id','movie_id']] = ratings_nf[['user_id','movie_id']].astype(int)\n",
    "movies_nf = pd.read_parquet('../../Data/Netflix/Netflix_movies.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a helper dictonary, for demonstration later on\n",
    "title_dict = dict(zip(movies_ml['movieId'], movies_ml['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **``Recsys 1:`` KNN Collaborative Item-Based**\n",
    "##### Collaborative Filtering **Without** item Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "The first system is the system without features as explained in the introduction. The system is made to be used on any type of dataset, if the init arguments are set correctly. Among the arguments is the type specification, by default its set to regression, but will later be changed during the comparison section. A few notes about the system. First of all this system does not utilize a similarity matrix, as it we found that it slows down the performance of the system, while not producing better results. The KNN system, finds the similar items to the input vector based on the user-item matrix. For this user-item matrix a CSR or sparse matrix is used. It saves the matrix in a 3 row format, significantly saving memory, especially with the sparse matrices we are dealing with. These factors lead to the fact that this system, can use the 24 million dataset of Netflix, and still is able to be fitted and get quick recommendations (2 a 3 seconds) on a medium laptop. We tested this also with a system using a similarity matrix, and it took significantly longer. Finally, both the systems are build in an object oriented approach. Apart from the benefits of easy transfering of variables it also allows for effeciently use later on. All the functions within the class are explained first in multiline comment within the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression / Classification Recsys without features\n",
    "\n",
    "class KNN_CF_ITEM:\n",
    "\n",
    "    def __init__(self, userField, itemField, valueField, type='regression'):\n",
    "        self.userField = userField\n",
    "        self.itemField = itemField\n",
    "        self.valueField = valueField\n",
    "        self.predict_type = type\n",
    "        self.set_distance_metrics()\n",
    "        self.set_n_recommendations_and_k()\n",
    "\n",
    "    '''\n",
    "    args of recommender:\n",
    "    userfield   : Name of column in which userid is located\n",
    "    itemfield   : Name of column in which itemid is located\n",
    "    valuefield  : Name of column in which ratings are located\n",
    "    type        : Type of prediction, either regression or classification\n",
    "    '''\n",
    "\n",
    "    def set_distance_metrics(self, primary_metric = 'cosine', prediction_metric = 'manhattan'):\n",
    "        '''set initial distance metrics'''\n",
    "        self.primary_distance = primary_metric\n",
    "        self.prediction_metric = prediction_metric\n",
    "\n",
    "    def set_n_recommendations_and_k(self, n=20, k=10):\n",
    "        '''Set initial parameters'''\n",
    "        self.n_neighbors = n\n",
    "        self.k = k\n",
    "\n",
    "    def create_matrix(self, data):\n",
    "        ''' First making some maps, as csr matrix requires range of 0 to N, with increments of 1.\n",
    "        the id's might not comply to this (e.g. gaps). So mapping is made for both user/item to index\n",
    "        and inverse. Secondly CSR matrix is made. A CSR matrix is chosen because of the sparsity.\n",
    "        it makes the recommender perform faster, compared to normal pivot table'''\n",
    "\n",
    "        #Mapping\n",
    "        N = data[self.userField].nunique()\n",
    "        M = data[self.itemField].nunique()\n",
    "        user_list = np.unique(data[self.userField])\n",
    "        item_list = np.unique(data[self.itemField])\n",
    "        self.user_to_index = dict(zip(user_list, range(0, N)))\n",
    "        self.item_to_index = dict(zip(item_list, range(0,M)))\n",
    "        self.index_to_item = dict(zip(range(0,M), item_list))\n",
    "        user_index = [self.user_to_index[i] for i in data[self.userField]]\n",
    "        item_index = [self.item_to_index[i] for i in data[self.itemField]]\n",
    "\n",
    "        #Create CSR matrix, items on rows, users on columns\n",
    "        self.matrix = csr_matrix((data[self.valueField], (item_index, user_index)), shape=(M, N))\n",
    "\n",
    "    def rated_by_user(self, user):\n",
    "        '''Helper function, to get the already rated items of the user. From this the favourite\n",
    "        items will be taken later on, which will be the input for finding similar items. Secondly,\n",
    "        this is later used to filter out the items already rated. '''\n",
    "\n",
    "        user_items = self.matrix.getcol(self.user_to_index[user]).A #Get vector of user, with ratings\n",
    "        rated_items = list(zip(np.where(user_items > 0)[0],user_items[user_items> 0])) #combine index and rating when above 0\n",
    "        self.rated_items = sorted(rated_items, key=lambda x: x[1], reverse=True) #sort the list on the ratings\n",
    "        self.avg_user = np.mean([item[1] for item in self.rated_items]) #Take average of user, for later\n",
    "\n",
    "\n",
    "    def fit(self, metric=None, k=None):\n",
    "        '''Fit the nearest neighbors model with the matrix. Nearest neighbors was chosen over a\n",
    "        similarity matrix due to faster performance, especially in combination with CSR'''\n",
    "        \n",
    "        if metric is None:\n",
    "            metric = self.primary_distance\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "        self.KNN = NearestNeighbors(n_neighbors=k, algorithm='brute',metric= metric)\n",
    "        self.KNN.fit(self.matrix) \n",
    "    \n",
    "\n",
    "    def find_similar_items(self, user):\n",
    "        \"\"\" Based on the favourite items of the user, we recommend similar items.     \n",
    "        To find enough items that the user has NOT rated yet. We will loop through\n",
    "        his/her favourite movies untill we have 10 items that the user has not seen yet\n",
    "        to create a pool of unseen movies. this ensure us that we do not end up with 0 \n",
    "        recommended items because the user has already seen them all.\"\"\"\n",
    "\n",
    "        self.rated_by_user(user) #run function to get favourite movies\n",
    "        self.favorite_indices = [item[0] for item in self.rated_items] #Retreive the favorite indeces\n",
    "        unseen_idx = [] #init pool in which items that has not been seen will be stored\n",
    "\n",
    "        for favorite_index in self.favorite_indices: # Loop through favorite items of user\n",
    "            item_vector = self.matrix[favorite_index].reshape(1, -1) # Get the vector\n",
    "            distances, indices = self.KNN.kneighbors(item_vector, n_neighbors=self.n_neighbors) # Find similar items\n",
    "            combined_list = list(zip(indices[0].tolist(), distances[0].tolist()))[1:] #combine and drop 1st\n",
    "            filtered_list = [(index, distance) for index, distance in combined_list if index not in self.favorite_indices] #filter out the ones user has rated\n",
    "            \n",
    "            for index, distance in filtered_list: #Loop through the filtered list of unseen items\n",
    "                if index not in [item[0] for item in unseen_idx]: #Check if not already in pool of items\n",
    "                   unseen_idx.append((index, distance)) #Add to unseen pool of items\n",
    "            if len(unseen_idx) >= self.n_neighbors: #Stop if we have enough recommendations\n",
    "                break #stop if we have enough unwatched items\n",
    "        \n",
    "        #store the (index, distance) of similar (unseen) items to  his/her favourites\n",
    "        self.similar_items = unseen_idx \n",
    "\n",
    "\n",
    "    def ratings_similar_items(self,user, n=100):\n",
    "        '''Now that we have the N similar items of the favourite items of the user. We look into\n",
    "        items that the user has ALREADY rated, and are similar to the ones we are going to recommend.\n",
    "        This will give us a good indication of how the user is going to rate the unseen item, that\n",
    "        we are going to recommend. We retreive those ratings and distance, to calculate the prediction\n",
    "        later on. Secondly as we are looking for neighbors of the top N recommendations, we can assign\n",
    "        an additional distance metric to hypertune. For this distance calculation we use our own made\n",
    "        package for calculating the distances'''\n",
    "\n",
    "        distance_calc = Distance()      #init our distance package\n",
    "        result_list = []                #init list to store results\n",
    "\n",
    "        #We look into the already rated items, and pick the ones that are closest to the unseen item\n",
    "        for index, _ in self.similar_items: \n",
    "            input_vector = self.matrix[index].A[0]\n",
    "            distances = []\n",
    "            for idx, rating in self.rated_items[:n]:\n",
    "                target_vector = self.matrix[idx].A[0]\n",
    "                distance = distance_calc.calculate(vector1 = input_vector, \n",
    "                                                   vector2 = target_vector, \n",
    "                                                   metric=self.prediction_metric)\n",
    "                distances.append((idx, distance, rating))\n",
    "            distances = sorted(distances, key=lambda x: x[1])[:10] #sort the list on the distance\n",
    "            result_list.append((index, distances))\n",
    "\n",
    "        self.ratings_neighbors = result_list\n",
    "        return result_list\n",
    "    \n",
    "\n",
    "    def predict_ratings(self):\n",
    "        '''This function will go through the ratings of the user for the neighbours of the N recommended items.\n",
    "        Next, there a two ways of predicting the ratings for the top N unseen items (Nikolakopoulos et al. 2021).\n",
    "\n",
    "        1.Regression:\n",
    "        if chosen for regression a prediction will be made for recommended item X by calculating a \n",
    "        weighted score. We take the scores that the user has given to items that are very similar to item X.\n",
    "        Then, by using the distance, we come to a weighted score for item X. This is less risky to \n",
    "        classification, as the final rating is based on more ratings, and therefore a safer bet.\n",
    "\n",
    "        Formula:  sum(w1 * rating1 + w2 * rating2) / sum(w1 + w2)\n",
    "\n",
    "        2.Classification:\n",
    "        if chosen for classification. Instead of calculating a weighted average, we take the rating of the\n",
    "        item that is most similar to the unseen item we are going to recommend. This is riskier, but can\n",
    "        actually give better results. Especially, if the discrete rating scale (e.g. 1 to 5) is a small range.\n",
    "        Its less recommended for ratings scales like 1 to 25. Choosing the rating that is closest, and classify\n",
    "        that one as the most appropriate (weight 1 or 0), has been proven to be a viable approach in literature.'''\n",
    "          \n",
    "        # 1. Regression prediction method\n",
    "        if self.predict_type == 'regression':\n",
    "            weighted_averages = [] \n",
    "            for idx, idx_dist_rating in self.ratings_neighbors:\n",
    "                weighted_sum = sum_inverse_distances = 0\n",
    "                for idx_nb, distance, rating in idx_dist_rating:\n",
    "                    if distance != 0:\n",
    "                        inverse_distance = 1 / distance\n",
    "                        weighted_sum += inverse_distance * rating\n",
    "                        sum_inverse_distances += inverse_distance\n",
    "                if sum_inverse_distances != 0:\n",
    "                    weighted_avg = weighted_sum / sum_inverse_distances\n",
    "                else:\n",
    "                    weighted_avg = self.avg_user #predict avg of user because no data (e.g. only rated 1 item)\n",
    "                weighted_averages.append((self.target_user, idx, weighted_avg))\n",
    "            self.recommendations_predictions = sorted(weighted_averages, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "\n",
    "        # 2. Classification Prediction Method\n",
    "        elif self.predict_type == 'classification':\n",
    "            sorted_voters = [] #list of sorted, and only the top voters (with weight 1 not 0)\n",
    "\n",
    "            for item in self.ratings_neighbors:\n",
    "                idx, entries = item #each item in recommendations format:\n",
    "                entries.sort(key=lambda x: x[1]) #sort on weight (distance)\n",
    "                top_vote = entries[0] #Take the one with the highest weight\n",
    "                sorted_voters.append((idx, top_vote)) #take the rating of the top vote\n",
    "            \n",
    "            #append user idx, recommended item and predicted score to the list\n",
    "            idx_and_predictions = []\n",
    "            for item in sorted_voters:\n",
    "                idx_and_predictions.append((self.target_user, item[0], item[1][2]))\n",
    "        \n",
    "            #Sort the list, so the indices with highest pred ratings are on top\n",
    "            self.recommendations_predictions = sorted(idx_and_predictions, key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        #If not chosen for either regression or classification, display error.\n",
    "        else:\n",
    "            print('Please select valid prediction type (regression or classification)')\n",
    "\n",
    "\n",
    "    def recommend(self, userlist, prints=True):\n",
    "        ''' This is the main function that combines most helper functions to recommend N new items \n",
    "        to a user, with the predicted ratings. This function will convert them also back to ID's\n",
    "        from indices, as its easier to evaluate '''  \n",
    "\n",
    "        recommendations = []\n",
    "        for user in userlist:\n",
    "            self.target_user = user\n",
    "            self.find_similar_items(user)\n",
    "            self.ratings_similar_items(user)\n",
    "            self.predict_ratings()\n",
    "            recommendations.append([(userid, self.index_to_item[idx], rating) for userid, idx, rating in self.recommendations_predictions][:self.n_neighbors]) #convert back to ID's   \n",
    "        recommendations = [sorted(sublist, key=lambda x: x[2], reverse=True) for sublist in recommendations]\n",
    "        if prints == True:\n",
    "            print(recommendations)\n",
    "        else:\n",
    "            return recommendations\n",
    "        \n",
    "\n",
    "    def evaluate(self, data, limit=0.5, prints=False):\n",
    "        ''' This is the evaluate function, the input should be the test data, if called manually.\n",
    "        The hypertune function will use the validation data automatically. This function compares\n",
    "        the predicted rating for the unseen items based on the training set, with the actual ratings\n",
    "        in the test (or validation) data set. It returns RMSE & MAE.\n",
    "\n",
    "        data    :   Actual data (usally test or validation in case of hypertune function)\n",
    "        limit   :   Manual limit to limit amount of users to be tested (for big testdata)\n",
    "        prints  :   Default is True, if True it outputs the results, for hypertune its turned Off\n",
    "        '''\n",
    "\n",
    "        user_list = data[self.userField].unique()\n",
    "        self.count = 0\n",
    "        if limit is not None:\n",
    "            user_list = user_list[:int(len(user_list) * limit)]\n",
    "        if prints == True: #If True output the prints for information\n",
    "            print(len(user_list), f' : Users are going to be evaluted. {limit} of input data')\n",
    "        predictions, targets = [], []\n",
    "        for user in user_list:\n",
    "            output = self.recommend([user], prints=False)\n",
    "            for rec in output:\n",
    "                for item in rec:\n",
    "                    actual_rating = actual_rating = data[(data[self.userField] == item[0]) & (data[self.itemField] == item[1])][self.valueField].values\n",
    "                    if (actual_rating.size > 0 & (item[2] > 0)):\n",
    "                        self.count += 1\n",
    "                        targets.append(actual_rating[0]) #store actual rating\n",
    "                        predictions.append(item[2])\n",
    "        rmse = np.sqrt(mean_squared_error(predictions, targets))\n",
    "        mae = mean_absolute_error(predictions, targets)\n",
    "        \n",
    "        if prints == True: #if True print the RMSE score\n",
    "            print('RMSE: '.ljust(20), round(rmse, 3))\n",
    "            print('MAE: '.ljust(20), round(mae, 3))\n",
    "            print(f'{self.count} Valid Ratings Evaluated')\n",
    "        else: # else output the rmse\n",
    "            return rmse\n",
    "        \n",
    "    def hypertune(self, data, k_folds=5, prints=True, limit=0.5):\n",
    "        ''' Hyperparameter Tuning function. Utilizes the evaluation function with a K-Fold\n",
    "        cross validation approach. One of the K fold acts as validation dataset to calculate\n",
    "        the RMSE for the particular parameters. This is done K times for each parameter combination.\n",
    "\n",
    "        data    :   (Train) Data to perform hypertuning on,\n",
    "        k_folds :   How many times to split the train data, and cross validate\n",
    "        prints  :   If True, outputs best metrics and RMSE found\n",
    "        '''\n",
    "\n",
    "        #Default set of parameters to test\n",
    "        k_list = [10,15,20]\n",
    "        n_rec_metric = ['cosine', 'manhattan', 'euclidean']\n",
    "        prediction_metric = ['manhattan','cosine', 'euclidean']\n",
    "\n",
    "        #dictonary to store the results and best_rmse that will get updated\n",
    "        best_params_dict = {}\n",
    "        best_rmse = 999\n",
    "\n",
    "        #Loop through every combination, and for each perform K-Fold cross validation\n",
    "        for k in k_list:\n",
    "            for rec_dist in n_rec_metric:\n",
    "                for pred_dist in prediction_metric:\n",
    "                    kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "                    rmse_scores = []\n",
    "                    for train_idx, validation_idx in kf.split(data):\n",
    "                        train_data = data.iloc[train_idx]\n",
    "                        val_data = data.iloc[validation_idx]\n",
    "                        self.create_matrix(train_data)\n",
    "                        self.set_distance_metrics(primary_metric=rec_dist, \n",
    "                                                prediction_metric=pred_dist)\n",
    "                        self.set_n_recommendations_and_k(k=k)\n",
    "                        self.fit()\n",
    "                        rmse = self.evaluate(val_data, limit=limit, prints=False)\n",
    "                        rmse_scores.append(rmse)\n",
    "                    avg_rmse = np.mean(rmse_scores)\n",
    "                    if avg_rmse < best_rmse:\n",
    "                        best_rmse = avg_rmse\n",
    "                        best_params_dict = {'K':k,\n",
    "                                            'primary_metric': rec_dist,\n",
    "                                            'prediction_metric': pred_dist,\n",
    "                                            'RMSE': round(best_rmse,3)}\n",
    "        \n",
    "        #update the system with new best parameters\n",
    "        self.set_n_recommendations_and_k(k=best_params_dict['K'])\n",
    "        self.set_distance_metrics(prediction_metric=best_params_dict['prediction_metric'],\n",
    "                                  primary_metric=best_params_dict['primary_metric'])\n",
    "\n",
    "\n",
    "        #Print the best results and parameters found\n",
    "        if prints == True:\n",
    "            print(f\"Best RMSE: {round(best_rmse, 3)}\")\n",
    "            print(\"-------------------\")\n",
    "            print(\"Best Parameters:\")\n",
    "            for param, value in best_params_dict.items():\n",
    "                print(f\"{param}:\".ljust(30) + f\"{value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **``Recsys 2:`` KNN Collaborative Item-Based**\n",
    "##### Collaborative Filtering **With** Features Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "The second system is the system WITH item features. This requires an additional step and argument compared to the other system. This system needs an item feature matrix as input, in addition to the ratings data. The item feature matrix are made first below, by using a simple one-hot encoding technique to get the genres per movie. This is done for both the movielens and netflix data. Futhermore, this system shares alot of similarities with the other system, only the process of creating the matrix has been changed to incorperate the addition of the hot encoded genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1 Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a feature matrix as input for additional features next to collaborative (MovieLens)\n",
    "movies_ml = movies_ml[['movieId', 'genres']]\n",
    "movies_ml.columns = ['movie_id', 'genres']\n",
    "genre_one_hot = movies_ml['genres'].str.get_dummies(\"|\")\n",
    "item_features_ml = pd.concat([movies_ml,genre_one_hot],axis=1).drop(columns='genres', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a feature matrix as input for additional features next to collaborative (Netflix)\n",
    "movies_nf = movies_nf[['movie_id', 'genres']]\n",
    "genre_one_hot = movies_nf['genres'].str.get_dummies(\"|\")\n",
    "item_features_nf = pd.concat([movies_nf,genre_one_hot],axis=1).drop(columns='genres', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 RecSys with Features added**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression / Classification Recsys with features\n",
    "\n",
    "class KNN_CF_ITEM_FEATURES:\n",
    "\n",
    "\n",
    "    def __init__(self, userField, itemField, valueField, featurematrix, type='regression'):\n",
    "        self.userField = userField\n",
    "        self.itemField = itemField\n",
    "        self.valueField = valueField\n",
    "        self.feature_matrix = featurematrix\n",
    "        self.predict_type = type\n",
    "        self.set_distance_metrics()\n",
    "        self.set_n_recommendations_and_k()\n",
    "        \n",
    "    def set_distance_metrics(self, primary_metric = 'cosine', prediction_metric = 'manhattan'):\n",
    "        ''' Function to set the default distance metrics, can be called later (by hypertune)\n",
    "        to set the distance metrics to something different.'''\n",
    "        self.primary_distance = primary_metric\n",
    "        self.prediction_metric = prediction_metric\n",
    "    \n",
    "    def set_n_recommendations_and_k(self,n=10, k=10):\n",
    "        self.n_neighbors = n\n",
    "        self.k = k\n",
    "\n",
    "    def create_matrix(self, data):\n",
    "        ''' As with the other system this function creates some usefull maps. However\n",
    "        for this system it also includes combining the item vectors (genre) with the\n",
    "        normal rating vectors and storing them in a combined matrix.'''\n",
    "\n",
    "        N = data[self.userField].nunique()\n",
    "        M = data[self.itemField].nunique()\n",
    "        user_list = np.unique(data[self.userField])\n",
    "        item_list = np.unique(data[self.itemField])\n",
    "        self.user_to_index = dict(zip(user_list, range(0, N)))\n",
    "        self.item_to_index = dict(zip(item_list, range(0,M)))\n",
    "        self.index_to_user = dict(zip(range(0,N), user_list))\n",
    "        self.index_to_item = dict(zip(range(0,M), item_list))\n",
    "        user_index = [self.user_to_index[i] for i in data[self.userField]]\n",
    "        item_index = [self.item_to_index[i] for i in data[self.itemField]]\n",
    "\n",
    "        #Create CSR matrix, items on rows, users on columns\n",
    "        self.rating_matrix = csr_matrix((data[self.valueField], (item_index, user_index)), shape=(M, N))\n",
    "\n",
    "        #combine vectors features and ratings\n",
    "        combined_item_vectors = np.zeros((M, N + (self.feature_matrix.shape[1]-1)))\n",
    "\n",
    "        # For each item in the standard user-item matrix, combine the vector for that item taken from the \n",
    "        # Feature matrix, with the vectors of the hot-encoded genres\n",
    "        for item_id in item_list:\n",
    "            item_index = self.item_to_index[item_id] #Get the item index from the dictonary\n",
    "            rating_vector = self.rating_matrix[item_index].toarray().flatten()  # Convert to dense array\n",
    "            feature_vector = self.feature_matrix[self.feature_matrix[self.itemField] == item_id].values[:, 1:].flatten() #Get vector from feature matrix for the specific item\n",
    "            combined_vector = np.concatenate((rating_vector, feature_vector)) #combine rating vector with genre vector\n",
    "            combined_item_vectors[item_index, :len(combined_vector)] = combined_vector #Store the combined vector in the combined matrix.\n",
    "        self.combined_matrix = combined_item_vectors\n",
    "\n",
    "    def rated_by_user(self, user):\n",
    "        #Helper function to get items already rated by user\n",
    "        user_items = self.rating_matrix.getcol(self.user_to_index[user]).A #Get vector of user, with ratings\n",
    "        rated_items = list(zip(np.where(user_items > 0)[0],user_items[user_items> 0])) #combine index and rating when above 0\n",
    "        self.rated_items = sorted(rated_items, key=lambda x: x[1], reverse=True) #sort the list on the ratings\n",
    "        self.avg_user = np.mean([item[1] for item in self.rated_items])\n",
    "\n",
    "    def fit(self, metric = None, k = None):\n",
    "        if metric is None:\n",
    "            metric = self.primary_distance\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "        self.KNN = NearestNeighbors(n_neighbors=k, algorithm='brute',metric= metric)\n",
    "        self.KNN.fit(self.combined_matrix)\n",
    "\n",
    "    def find_similar_items(self, user):\n",
    "        #Helper function to find k similar items\n",
    "\n",
    "        \"\"\" To find enough items that the user has NOT rated yet. We will loop through\n",
    "        his/her favourite movies untill we have 10 movies that the user has not seen yet\n",
    "        to create a pool of unseen movies. this ensure us that we do not end up with 0 \n",
    "        recommended items because the user has already seen them all.\"\"\"\n",
    "\n",
    "        self.rated_by_user(user) #run function to get favourite movies\n",
    "        self.favorite_indices = [item[0] for item in self.rated_items]\n",
    "        unseen_idx = [] #init pool in which items that has not been seen will be stored\n",
    "\n",
    "\n",
    "        for favorite_index in self.favorite_indices:\n",
    "            item_vector = self.combined_matrix[favorite_index].reshape(1, -1)\n",
    "            distances, indices = self.KNN.kneighbors(item_vector, n_neighbors=self.n_neighbors)\n",
    "            combined_list = list(zip(indices[0].tolist(), distances[0].tolist()))[1:] #combine and drop 1st\n",
    "            filtered_list = [(index, distance) for index, distance in combined_list if index not in self.favorite_indices]\n",
    "            for index, distance in filtered_list:\n",
    "                if index not in [item[0] for item in unseen_idx]:\n",
    "                   unseen_idx.append((index, distance))\n",
    "            if len(unseen_idx) >= self.n_neighbors:\n",
    "                break #stop if we have enough unwatched movies\n",
    "        \n",
    "        self.similar_items = unseen_idx\n",
    "        return self.similar_items\n",
    "    \n",
    "    def ratings_similar_items(self,user, n=100):\n",
    "        '''Now that we have the N similar items of the favourite items of the user. We look into\n",
    "        items that the user has ALREADY rated, and are similar to the ones we are going to recommend.\n",
    "        This will give us a good indication of how the user is going to rate the unseen item, that\n",
    "        we are going to recommend. We retreive those ratings and distance, to calculate the prediction\n",
    "        later on. Secondly as we are looking for neighbors of the top N recommendations, we can assign\n",
    "        an additional distance metric to hypertune. For this distance calculation we use our own made\n",
    "        package for calculating the distances'''\n",
    "\n",
    "        distance_calc = Distance()      #init our distance package\n",
    "        result_list = []                #init list to store results\n",
    "\n",
    "        #We look into the already rated items, and pick the ones that are closest to the unseen item\n",
    "        for index, _ in self.similar_items: \n",
    "            input_vector = self.combined_matrix[index]\n",
    "            distances = []\n",
    "            for idx, rating in self.rated_items[:n]:\n",
    "                target_vector = self.combined_matrix[idx]\n",
    "                distance = distance_calc.calculate(vector1 = input_vector, \n",
    "                                                   vector2 = target_vector, \n",
    "                                                   metric=self.prediction_metric)\n",
    "                distances.append((idx, distance, rating))\n",
    "            distances = sorted(distances, key=lambda x: x[1])[:10] #sort the list on the distance\n",
    "            result_list.append((index, distances))\n",
    "\n",
    "        self.ratings_neighbors = result_list\n",
    "        return result_list\n",
    "\n",
    "    def predict_ratings(self):\n",
    "        '''This function will go through the ratings of the user for the neighbours of the N recommended items.\n",
    "        Next, there a two ways of predicting the ratings for the top N unseen items (Nikolakopoulos et al. 2021).\n",
    "\n",
    "        1.Regression:\n",
    "        if chosen for regression a prediction will be made for recommended item X by calculating a \n",
    "        weighted score. We take the scores that the user has given to items that are very similar to item X.\n",
    "        Then, by using the distance, we come to a weighted score for item X. This is less risky to \n",
    "        classification, as the final rating is based on more ratings, and therefore a safer bet.\n",
    "\n",
    "        Formula:  sum(w1 * rating1 + w2 * rating2) / sum(w1 + w2)\n",
    "\n",
    "        2.Classification:\n",
    "        if chosen for classification. Instead of calculating a weighted average, we take the rating of the\n",
    "        item that is most similar to the unseen item we are going to recommend. This is riskier, but can\n",
    "        actually give better results. Especially, if the discrete rating scale (e.g. 1 to 5) is a small range.\n",
    "        Its less recommended for ratings scales like 1 to 25. Choosing the rating that is closest, and classify\n",
    "        that one as the most appropriate (weight 1 or 0), has been proven to be a viable approach in literature.'''\n",
    "          \n",
    "        # 1. Regression prediction method\n",
    "        if self.predict_type == 'regression':\n",
    "            weighted_averages = [] \n",
    "            for idx, idx_dist_rating in self.ratings_neighbors:\n",
    "                weighted_sum = sum_inverse_distances = 0\n",
    "                for idx_nb, distance, rating in idx_dist_rating:\n",
    "                    if distance != 0:\n",
    "                        inverse_distance = 1 / distance\n",
    "                        weighted_sum += inverse_distance * rating\n",
    "                        sum_inverse_distances += inverse_distance\n",
    "                if sum_inverse_distances != 0:\n",
    "                    weighted_avg = weighted_sum / sum_inverse_distances\n",
    "                else:\n",
    "                    weighted_avg = self.avg_user #predict avg of user because no data (e.g. only rated 1 item)\n",
    "                weighted_averages.append((self.target_user, idx, weighted_avg))\n",
    "            self.recommendations_predictions = sorted(weighted_averages, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "\n",
    "        # 2. Classification Prediction Method\n",
    "        elif self.predict_type == 'classification':\n",
    "            sorted_voters = [] #list of sorted, and only the top voters (with weight 1 not 0)\n",
    "\n",
    "            for item in self.ratings_neighbors:\n",
    "                idx, entries = item #each item in recommendations format:\n",
    "                entries.sort(key=lambda x: x[1]) #sort on weight (distance)\n",
    "                top_vote = entries[0] #Take the one with the highest weight\n",
    "                sorted_voters.append((idx, top_vote)) #take the rating of the top vote\n",
    "            \n",
    "            #append user idx, recommended item and predicted score to the list\n",
    "            idx_and_predictions = []\n",
    "            for item in sorted_voters:\n",
    "                idx_and_predictions.append((self.target_user, item[0], item[1][2]))\n",
    "        \n",
    "            #Sort the list, so the indices with highest pred ratings are on top\n",
    "            self.recommendations_predictions = sorted(idx_and_predictions, key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        #If not chosen for either regression or classification, display error.\n",
    "        else:\n",
    "            print('Please select valid prediction type (regression or classification)')\n",
    "\n",
    "    \n",
    "    def recommend(self, userlist, prints=True):\n",
    "        ''' This is the main function that combines most helper functions to recommend N new items \n",
    "        to a user, with the predicted ratings. This function will convert them also back to ID's\n",
    "        from indices '''  \n",
    "        n_neighbors = self.n_neighbors\n",
    "        recommendations = []\n",
    "        for user in userlist:\n",
    "            self.target_user = user\n",
    "            self.find_similar_items(user)\n",
    "            self.ratings_similar_items(user)\n",
    "            self.predict_ratings()\n",
    "            recommendations.append([(userid, self.index_to_item[idx], rating) for userid, idx, rating in self.recommendations_predictions][:n_neighbors]) #convert back to ID's\n",
    "        recommendations = [sorted(sublist, key=lambda x: x[2], reverse=True) for sublist in recommendations]\n",
    "        if prints == True:\n",
    "            print(recommendations)\n",
    "        else:\n",
    "            return recommendations\n",
    "        \n",
    "    def evaluate(self, data, limit=0.5, prints=False):\n",
    "        ''' This is the evaluate function, the input should be the test data, if called manually.\n",
    "        The hypertune function will use the validation data automatically. This function compares\n",
    "        the predicted rating for the unseen items based on the training set, with the actual ratings\n",
    "        in the test (or validation) data set. It returns RMSE & MAE.\n",
    "\n",
    "        data    :   Actual data (usally test or validation in case of hypertune function)\n",
    "        limit   :   Manual limit to limit amount of users to be tested (for big testdata)\n",
    "        prints  :   Default is True, if True it outputs the results, for hypertune its turned Off\n",
    "        '''\n",
    "\n",
    "        user_list = data[self.userField].unique()\n",
    "        self.count = 0\n",
    "        if limit is not None:\n",
    "            user_list = user_list[:int(len(user_list) * limit)]\n",
    "        if prints == True: #If True output the prints for information\n",
    "            print(len(user_list), f' : Users are going to be evaluted. {limit} of input data')\n",
    "        predictions, targets = [], []\n",
    "        for user in user_list:\n",
    "            output = self.recommend([user], prints=False)\n",
    "            for rec in output:\n",
    "                for item in rec:\n",
    "                    actual_rating = actual_rating = data[(data[self.userField] == item[0]) & (data[self.itemField] == item[1])][self.valueField].values\n",
    "                    if (actual_rating.size > 0 & (item[2] > 0)):\n",
    "                        self.count += 1\n",
    "                        targets.append(actual_rating[0]) #store actual rating\n",
    "                        predictions.append(item[2])\n",
    "        rmse = np.sqrt(mean_squared_error(predictions, targets))\n",
    "        mae = mean_absolute_error(predictions, targets)\n",
    "        \n",
    "        if prints == True: #if True print the RMSE score\n",
    "            print('RMSE: '.ljust(20), round(rmse, 3))\n",
    "            print('MAE: '.ljust(20), round(mae, 3))\n",
    "            print(f'{self.count} Valid Ratings Evaluated')\n",
    "        else: # else output the rmse\n",
    "            return rmse\n",
    "        \n",
    "    def hypertune(self, data, k_folds=5, prints=True, limit=0.5):\n",
    "        ''' Hyperparameter Tuning function. Utilizes the evaluation function with a K-Fold\n",
    "        cross validation approach. One of the K fold acts as validation dataset to calculate\n",
    "        the RMSE for the particular parameters. This is done K times for each parameter combination.\n",
    "\n",
    "        data    :   (Train) Data to perform hypertuning on,\n",
    "        k_folds :   How many times to split the train data, and cross validate\n",
    "        prints  :   If True, outputs best metrics and RMSE found\n",
    "        '''\n",
    "\n",
    "        #Default set of parameters to test\n",
    "        k_list = [10,15,20]\n",
    "        n_rec_metric = ['cosine', 'manhattan', 'euclidean']\n",
    "        prediction_metric = ['manhattan''cosine', 'euclidean']\n",
    "\n",
    "        #dictonary to store the results and best_rmse that will get updated\n",
    "        best_params_dict = {}\n",
    "        best_rmse = 999\n",
    "\n",
    "        #Loop through every combination, and for each perform K-Fold cross validation\n",
    "        for k in k_list:\n",
    "            for rec_dist in n_rec_metric:\n",
    "                for pred_dist in prediction_metric:\n",
    "                    kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "                    rmse_scores = []\n",
    "                    for train_idx, validation_idx in kf.split(data):\n",
    "                        train_data = data.iloc[train_idx]\n",
    "                        val_data = data.iloc[validation_idx]\n",
    "                        self.create_matrix(train_data)\n",
    "                        self.set_distance_metrics(primary_metric=rec_dist, \n",
    "                                                prediction_metric=pred_dist)\n",
    "                        self.set_n_recommendations_and_k(k=k)\n",
    "                        self.fit()\n",
    "                        rmse = self.evaluate(val_data, limit=limit, prints=False)\n",
    "                        rmse_scores.append(rmse)\n",
    "                    avg_rmse = np.mean(rmse_scores)\n",
    "                    if avg_rmse < best_rmse:\n",
    "                        best_rmse = avg_rmse\n",
    "                        best_params_dict = {'K':k,\n",
    "                                            'primary_metric': rec_dist,\n",
    "                                            'prediction_metric': pred_dist,\n",
    "                                            'RMSE': round(best_rmse,3)}\n",
    "        \n",
    "        #update the system with new best parameters\n",
    "        self.set_n_recommendations_and_k(k=best_params_dict['K'])\n",
    "        self.set_distance_metrics(prediction_metric=best_params_dict['prediction_metric'],\n",
    "                                  primary_metric=best_params_dict['primary_metric'])\n",
    "\n",
    "\n",
    "        #Print the best results and parameters found\n",
    "        if prints == True:\n",
    "            print(f\"Best RMSE: {round(best_rmse, 3)}\")\n",
    "            print(\"-------------------\")\n",
    "            print(\"Best Parameters:\")\n",
    "            for param, value in best_params_dict.items():\n",
    "                print(f\"{param}:\".ljust(30) + f\"{value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Example Usage**\n",
    "Before going to the comparisons of the models, we will show an example usage of the recommender, to show some of its features. We will show an example using, the regression methodology on the movielens data. We get the recommendations for a user, based on his/her favorite movie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Train, test split**\n",
    "The train and test split is done via the traditional way. The reasoning behind this is that splitting it on e.g. time, resulted in to few actual interactions, to comprehensively evaluate the systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train Test movielens\n",
    "train_data, test_data = train_test_split(ratings_ml, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 Init the Recommender and Hypertune**\n",
    "We give the recommender the names of the columns in which the relevant items are located. Specify the type of prediction methodology it needs to use, and give it the train data. In the hypertune function this data is splitted again during the K-Fold crossvalidation into train and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE: 0.962\n",
      "-------------------\n",
      "Best Parameters:\n",
      "K:                            10\n",
      "primary_metric:               cosine\n",
      "prediction_metric:            cosine\n",
      "RMSE:                         0.962\n"
     ]
    }
   ],
   "source": [
    "#Init the recommender\n",
    "recommender = KNN_CF_ITEM('user_id','movie_id','rating', type='regression')\n",
    "recommender.create_matrix(train_data)\n",
    "recommender.fit()\n",
    "recommender.hypertune(train_data, limit=0.05, prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3 Get recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Unseen Movies:\n",
      "Predator 2 (1990)                                  4.85\n",
      "Death Wish 4: The Crackdown (1987)                 4.58\n",
      "Hollywood Knights, The (1980)                      4.58\n",
      "Bustin' Loose (1981)                               4.58\n",
      "Bronco Billy (1980)                                4.57\n",
      "Crimson Pirate, The (1952)                         4.57\n",
      "Black Sabbath (Tre volti della paura, I) (1963)    4.57\n",
      "Iron Eagle (1986)                                  4.54\n",
      "RoboCop (1987)                                     4.44\n",
      "Hellraiser (1987)                                  4.43\n"
     ]
    }
   ],
   "source": [
    "#Get recommendations for user 3, based on his/her favourite movies\n",
    "output = recommender.recommend([3], prints=False)\n",
    "\n",
    "print('Recommended Unseen Movies:')\n",
    "for _, item, rating in output[0][:10]:\n",
    "    movie = title_dict[item]\n",
    "    print(f'{movie}'.ljust(50), round(rating,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Comparing The Recommender Systems**\n",
    "Now in the following section we will compare the recommender systems. For each system and prediction methodology, we fit the system, hypertune it, and evaluate it on the test data. We split the comparison for the two datasets. Futhermore, on each dataset we will also try the classification vs regression methodology. This will give us some nice insights into which system works best (item features vs no item features) and which prediction methodology. The insights are discussed at the end of each dataset analysis. This is the structure for comparing the systems:  \n",
    "  \n",
    "**Movielens:**\n",
    "1) Regression system standard\n",
    "2) Classification system standard\n",
    "3) Regression system with item features\n",
    "4) Classification system with item features\n",
    "5) Findings\n",
    "\n",
    "**Netflix:**\n",
    "1) Regression system standard\n",
    "2) Classification system standard\n",
    "3) Regression system with item features\n",
    "4) Classification system with item features\n",
    "5) Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 Results `MovieLens`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will test the recommender systems and their type of rating prediciton (regression or classification) on the `movielens` dataset. For each recommender system we will input the necessary arguments, and specify the type of prediction methodology, and hypertune the model. Before that we will split the movielens data. We split the normal way, the reasoning behind this is that splitting it on e.g. time, resulted in to few actual interactions, to comprehensively evaluate the systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train Test movielens\n",
    "train_data_ml, test_data_ml = train_test_split(ratings_ml, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Recsys 1:`` Regression**  \n",
    "KNN item-based CF, without extra item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the recsys, with movielens traindata and type regression\n",
    "KNN_CF_1 = KNN_CF_ITEM('user_id', 'movie_id', 'rating', type='regression')\n",
    "KNN_CF_1.create_matrix(train_data_ml)\n",
    "KNN_CF_1.hypertune(train_data_ml, limit=0.5, k_folds=3, prints=False)\n",
    "KNN_CF_1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610  : Users are going to be evaluted. 1 of input data\n",
      "RMSE:                0.909\n",
      "MAE:                 0.682\n",
      "1255 Valid Ratings Evaluated\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the recommender on test data\n",
    "KNN_CF_1.evaluate(test_data_ml, limit=1, prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Recsys 2:`` Classification**  \n",
    "KNN item-based CF, without extra item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the recsys, with movielens traindata and type classification\n",
    "KNN_CF_2 = KNN_CF_ITEM('user_id', 'movie_id', 'rating', type='classification')\n",
    "KNN_CF_2.create_matrix(train_data_ml)\n",
    "KNN_CF_2.hypertune(train_data_ml, limit=0.5, k_folds=3, prints=False)\n",
    "KNN_CF_2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610  : Users are going to be evaluted. 1 of input data\n",
      "RMSE:                1.045\n",
      "MAE:                 0.716\n",
      "1384 Valid Ratings Evaluated\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the recommender on test data\n",
    "KNN_CF_2.evaluate(test_data_ml, limit=1, prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Recsys 3 With Features:`` Regression**  \n",
    "KNN item-based CF, with extra item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the recsys, with movielens traindata and type classification\n",
    "KNN_CF_3 = KNN_CF_ITEM_FEATURES('user_id', 'movie_id', 'rating', featurematrix=item_features_ml, type='regression')\n",
    "KNN_CF_3.create_matrix(train_data_ml)\n",
    "KNN_CF_3.hypertune(train_data_ml, limit=0.03, k_folds=3, prints=False)\n",
    "KNN_CF_3.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610  : Users are going to be evaluted. 1 of input data\n",
      "RMSE:                0.886\n",
      "MAE:                 0.673\n",
      "768 Valid Ratings Evaluated\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the recommender on test data\n",
    "KNN_CF_3.evaluate(test_data_ml, limit=1, prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Recsys 4 With Features:`` Classification**  \n",
    "KNN item-based CF, with extra item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the recsys, with movielens traindata and type classification\n",
    "KNN_CF_4 = KNN_CF_ITEM_FEATURES('user_id', 'movie_id', 'rating', featurematrix=item_features_ml, type='classification')\n",
    "KNN_CF_4.create_matrix(train_data_ml)\n",
    "KNN_CF_4.hypertune(train_data_ml, limit=0.03, k_folds=3, prints=False)\n",
    "KNN_CF_4.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610  : Users are going to be evaluted. 1 of input data\n",
      "RMSE:                0.986\n",
      "MAE:                 0.667\n",
      "815 Valid Ratings Evaluated\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the recommender on test data\n",
    "KNN_CF_4.evaluate(test_data_ml, limit=1, prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Findings:** Movielens \n",
    "  \n",
    "\n",
    "**Regression vs Classification:**\n",
    "For both with and without additional features, the classification had worse accuracy. This is understandable, as classification is a riskier approach. However it can also fall the other way, because if its correct, it will have a closer predicted rating than is possible with the regression methodology, as that is based and therefore spread among more ratings. However, because many ratings are evaluated, it might have an effect on the evaluation of the classification, as it will bring it closer to the average. In terms of speed, both methods are similar; both are able to give recommendations for 5 people within 0.2 seconds. Also, the evaluation takes around the same time for both ways of predicting ratings. Therefore, for this dataset, the traditional regression method is the preferred option, as it returns a higher RMSE while having no significant impact on performance.\n",
    "  \n",
    "**With vs Without additional Features:**  \n",
    "Secondly, let's take a look at the different results when using only ratings to find similar items versus using also the genres of the items. Here, we observe that by adding item features, we are able to achieve a lower RMSE (0.886 vs 0.909). Furthermore, we see the same trend in the classification task, where the model incorporating the extra features outperforms the one without (0.986 vs 1.045). However, a drawback of adding additional features is the impact on speed. Particularly during the fitting process, the time has increased from 0.3 seconds to 1.8 seconds, as a larger and additional feature matrix is required. Additionally, the recommendation time has also increased; when recommending for 10 users, the model that uses the extra features takes 0.2 seconds longer (0.8 seconds vs 1.0 second). This is something to keep in mind when scaling up to a larger dataset in the future. However, for this analysis. The system that incorperates additional item features, is the preferred system, which is expected as it has more information to base its similarity on. However, we should keep in mind that mixing content based principles with the collaborative approach has its drawbacks, that cannot be measured at this moment. It can be that due to the implementation of genres, people will get stuck with recommendetions within a specific genre, as we are moving more towards features then rating patterns as foundation. This might effect customer satisfaction and overall novelty of recommendations in the long run.\n",
    "  \n",
    "**Best Recommender system Movielens:**  \n",
    "Regression with additional features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4.2 Results `Netflix`**  \n",
    "Now we will do the same tests on the Netflix dataset. This dataset is 4x bigger, with also a 3.8 times the amount of users. The ratio item user is 7 to 1, while the movielens data is 15 to 1. (See EDA for more information). This means that Item-based should work in theory a bit better as its closer to its ideal scenario of having less items than users. We will now perform the same tests and write our conclusions below. Once again we start with splitting the Netflix data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train Test Netflix\n",
    "train_data_nf, test_data_nf = train_test_split(ratings_nf, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Recsys 1:`` Regression**  \n",
    "KNN item-based CF, without extra item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the recsys, with netflix traindata and type regression\n",
    "KNN_CF_N1 = KNN_CF_ITEM('user_id', 'movie_id', 'rating', type='regression')\n",
    "KNN_CF_N1.create_matrix(train_data_nf)\n",
    "KNN_CF_N1.hypertune(train_data_nf, limit=0.03, k_folds=3, prints=False)\n",
    "KNN_CF_N1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982  : Users are going to be evaluted. 0.5 of input data\n",
      "RMSE:                1.037\n",
      "MAE:                 0.73\n",
      "2779 Valid Ratings Evaluated\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the recommender on test data\n",
    "KNN_CF_N1.evaluate(test_data_nf, limit=0.5, prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Recsys 2:`` Classification**  \n",
    "KNN item-based CF, without extra item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the recsys, with netflix traindata and type classification\n",
    "KNN_CF_N2 = KNN_CF_ITEM('user_id', 'movie_id', 'rating', type='classification')\n",
    "KNN_CF_N2.create_matrix(train_data_nf)\n",
    "KNN_CF_N2.hypertune(train_data_nf, limit=0.03, k_folds=3, prints=False)\n",
    "KNN_CF_N2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786  : Users are going to be evaluted. 0.4 of input data\n",
      "RMSE:                1.091\n",
      "MAE:                 0.701\n",
      "2443 Valid Ratings Evaluated\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the recommender on test data\n",
    "KNN_CF_N2.evaluate(test_data_nf, limit=0.4, prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Recsys 3 With Features:`` Regression**  \n",
    "KNN item-based CF, with extra item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the recsys with features, with netflix traindata and type regression\n",
    "KNN_CF_N3 = KNN_CF_ITEM_FEATURES('user_id', 'movie_id', 'rating', featurematrix= item_features_nf, type='regression')\n",
    "KNN_CF_N3.create_matrix(train_data_nf)\n",
    "KNN_CF_N3.hypertune(train_data_nf, limit=0.03, k_folds=3, prints=False)\n",
    "KNN_CF_N3.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786 : Users are going to be evaluted. 0.4 of input data\n",
      "RMSE:                0.922\n",
      "MAE:                 0.634\n",
      "2503 Valid Ratings Evaluated\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the recommender on test data\n",
    "KNN_CF_N3.evaluate(test_data_nf, limit=0.4, prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Recsys 4 With Features:`` Classification**  \n",
    "KNN item-based CF, with extra item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the recsys with features, with netflix traindata and type classification\n",
    "KNN_CF_N4 = KNN_CF_ITEM_FEATURES('user_id', 'movie_id', 'rating', featurematrix=item_features_nf, type='classification')\n",
    "KNN_CF_N4.create_matrix(train_data_nf)\n",
    "KNN_CF_N4.hypertune(train_data_nf, limit=0.03, k_folds=3, prints=False)\n",
    "KNN_CF_N4.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786 : Users are going to be evaluted. 0.4 of input data\n",
      "RMSE:                0.935\n",
      "MAE:                 0.6\n",
      "2342 Valid Ratings Evaluated\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the recommender on test data\n",
    "KNN_CF_N4.evaluate(test_data_nf, limit=0.4, prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Findings:** Netflix\n",
    "  \n",
    "  \n",
    "**Regression vs Classification:**\n",
    "As with MovieLens, the classification performs slightly worse, but not to the same extent as observed with MovieLens. With the Netflix dataset, the classification methodology RMSE performance drop is actually negliable, but nevertheless it continues the trend that it performs worse than the regression method. In general this suggests that for these datasets, classification may not be the preferred method. One possibility would for future research would be to test a system that tests for which users this method works best and implement a mixed recommender that utilizes the most suitable methodology for certain types of users. Finally, similar to MovieLens, the performance remains comparable in terms of speed between the two predictions methodologies.\n",
    "  \n",
    "**With vs Without additional Features:**  \n",
    "As with MovieLens, using additional features results in a better RMSE. However, similar to MovieLens, fitting the model takes longer with the additional features (3 seconds with features, 0.3 seconds without). The model without features is almost unaffected by the larger dataset, while the model with features does experience some impact (1 second to 3 seconds). As a test, we applied the model without features to the Netflix dataset with 24,000,000 rows, and it remained very fast, taking only 3 seconds for recommendations on a medium laptop. Conversely, the model with features would struggle to perform at that scale, due to a bigger matrix. However, in this scenario, apart from fitting the model, the recommendation speed is negligibly different compared to the model without features. Therefore, as with MovieLens, the regression-based model with additional features is the preferred choice, but ofcourse has might have the same drawbacks in the long run as discussed in the findings of the movielens.\n",
    "  \n",
    "**Best Recommender system Netflix:**  \n",
    "regression based model with features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5 Final Conclusions** \n",
    "In general, the KNN-based item-based collaborative system performed reasonably well. As a benchmark, the off-the-shelf 'Surprise' algorithms achieved an RMSE of 0.98 on the MovieLens data, while this system often performed even better. Furthermore, some valuable insights have been gleaned regarding which type of system performs best. For both datasets, systems that incorporated extra item features and used regression methods for predicting ratings consistently outperformed others. Despite the small discrete rating scale of 1-5, classification didn't perform as well as expected in this analysis.Additionally, as mentioned, systems that included additional features performed better, albeit at a cost. The performance of utilizing an additional feature matrix slowed down, especially during fitting and to some extent in recommendation speed. Nonetheless, the systems generally performed quite well, capable of recommending items very quickly to multiple users, even on a medium-performing laptop. However, it's worth noting that when dealing with even larger datasets, the model incorporating features might experience slower performance compared to the alternative model. And could on the long run, restrict novelity of items recommended to users, as the system leans more towards a hybrid with content based, which are known to struggle with this issue. In conclusion, in this static analysis, for both datasets, a KNN-based algorithm that incorporates item features in addition to ratings, using a regression approach instead of classification, emerged as the most effective choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
